# Presentation for a paper about GPU

Parallel and accurate k-means algorithm on CPU-GPU architectures for spectral clustering

<https://onlinelibrary.wiley.com/doi/10.1002/cpe.6621>

---

- [Presentation for a paper about GPU](#presentation-for-a-paper-about-gpu)
  - [Summary](#summary)
  - [Contents](#contents)
    - [1. Introduction](#1-introduction)
      - [Notation - Table #1](#notation---table-1)
    - [2. A computational chain for spectral clustering](#2-a-computational-chain-for-spectral-clustering)
      - [2.1. Background for spectral clustering](#21-background-for-spectral-clustering)
      - [2.2. k-Means algorithm](#22-k-means-algorithm)
      - [2.3. Hybrid CPU-GPU complete processing chain](#23-hybrid-cpu-gpu-complete-processing-chain)
    - [3. Optimizing parallel k-Means algorithm](#3-optimizing-parallel-k-means-algorithm)
      - [3.1. Parallel implementation strategies](#31-parallel-implementation-strategies)
      - [3.2. ComputeAssign routine](#32-computeassign-routine)
      - [3.3. Update routine](#33-update-routine)
        - [3.3.1. Effect of rounding errors](#331-effect-of-rounding-errors)
        - [3.3.2. Two-step method with package processing](#332-two-step-method-with-package-processing)
    - [4. Experimental evaluation](#4-experimental-evaluation)
      - [4.1. Experiments on synthetic dataset](#41-experiments-on-synthetic-dataset)
      - [4.2. Experiments on real-world datasets](#42-experiments-on-real-world-datasets)
      - [4.3. Comparison with other recent parallel k-means implementations](#43-comparison-with-other-recent-parallel-k-means-implementations)
    - [5. Conclusion](#5-conclusion)
  - [Presentation plan](#presentation-plan)
    - [~~Subject(key points) to explain...~~ Split into 3](#subjectkey-points-to-explain-split-into-3)

---

## Summary

- In common, k-means is used for spectral clustering
- When dataset is large, the process suffers from lack of scalability
- To solve that, preprocessing the data with k-means can reduce the input data.
- This paper provides: Parallel optimization tech for k-means on CPU, GPU

## Contents

### 1. Introduction

- Clustering
  - means Grouping similar data into subset
  - is Important tasks in unsupervised ML and data mining
  - Has many applications
- k-means
  - Distance-based method
  - Effective in finding convex clusters, but not nonconvex clusters
    > Example drawing of Convex cluster and non-convex cluster
    >
    > ![img](https://i.stack.imgur.com/P7XOP.png)
    > <https://math.stackexchange.com/questions/2751592/what-defines-a-convex-cluster-and-how-it-differentiates-from-other-types>
  - Selection of initial cluster centroids is important to avoid some problems
- Spectral clustering
  - Based on Graph theory
  - Has advantages over k-means:
    - Can be applied in nonconvex clusters
    - Can automatically estimate the number of clusters in some cases with "eigengap heuristic"
- Implement spectral clustering on HPC(High Performance Computer)
  - Potential to be efficient, with good libraries
  - But critical in large scale, because cost is $O(n^3)$.
- Ways to be Scalable
  - Decreasing the computational complexity: Efficient but not generic
  - Decrease data size
  - Compute in parallel/distributed architecture
    - CPU-GPU heterogeneous platforms: CPU for big memory computing, GPU for computing power...?
- Optimize CPU and GPU
- Contents
  - 2 = Classical method of spectral clustering, heterogeneous CPU-GPU-based computational chain
  - 3 = Parallel implementations of k-means on CPU, GPU with optimizations
  - 4 = Experiment results
  - 5 = Conclusion

#### Notation - Table #1

| Notation | Meaning                                  |
| -------- | ---------------------------------------- |
| $n$      | Number of data instances                 |
| $n_d$    | Number of dimensions for each instance   |
| $k_c$    | Number of desired clusters               |
| $k_r$    | Number of _representatives_              |
| $x_i$    | Data instance $i$                        |
| $s_{ij}$ | Similarity between instances $i$ and $j$ |

### 2. A computational chain for spectral clustering

- k-means
- CPU-GPU processing chain for spectral clustering

#### 2.1. Background for spectral clustering

- Explains How to
  - Represent data instances as a graph
  - Represent the graph as a matrix, which is "graph Laplacian"
    > Laplacian matrix = Degree matrix - Adjacent matrix

...

-

#### 2.2. k-Means algorithm

#### 2.3. Hybrid CPU-GPU complete processing chain

### 3. Optimizing parallel k-Means algorithm

- 2 parallel/optimized implementation of the k-means on CPU, GPU
  - Including: inherent bottlenecks, suggested optimization methods in updating centroid

#### 3.1. Parallel implementation strategies

- For parellelization of the k-means
  - on CPU
    - Use OpenMP
    - Use autovectorization
    - minimazing cache misses
  - on GPU
    - Dev in CUDA
- For fast transfer between CPU-GPU
  - Minimize data transfers between CPU-GPU
  - Use pinned memory
  - Program (We already learned about this)
    1. transfer data from CPU to GPU at the beginning
    2. Launch CUDA kernels
    3. Transfer the cluster labels to CPU
- For coalescence(at once) of memory access
  - Transfer transposed matrices of
    - Data instances
    - centroids on GPU
      - It's okay because matrices is small
      - Use `cuBLAS.geam()`
- Number of changed should be transferred to CPU at each iteration, but the price of this is negligible
  - But, experimentally set optimal grid, block, thread sizes
- _ComputeAssign_ routine is nice
- _Update_ routine is bad, emitting rounding errors due to reduction operations

#### 3.2. ComputeAssign routine

- _ComputeAssign_ = distance compute + instance assign = minimized data access
- code
  - `#pragma omp`= OpenMP (`omp.h`)
    - parallel
    - for reduction = process `tract` variable and sum

```cpp
void main() {
#pragma omp parallel{
#pragma omp for reduction(+:track)
  for int(i = 0; i < n; i++) { // for instances
   int min = 0;
   T_real dist_sq, minDist_sq = FLT_MAX;
   for (int k = 0; k < kc; k++) { // for clusters
    dist_sq = 0.0f;
    for (int j = 0; j < nd; j++) { // for dimensions
     dist_sq += (data[i * nd + j] - cent[k][j])
      * (data[i * nd + j] - cent[k][j]);
     bool a = (dist_sq < minDist_sq); // if this distance is less then minimum found yet, then...
     min = (a ? k : min); // min = this cluster(label), else 0
     minDist_sq = (a ? dist_sq : minDist_sq); // update minimum found, else don't change
    }
   }
 
   if (label[i] != min) { // Label of the instance is changed
    label[i] = min; // Change the label
    track++ // Count it
   }
  }
  }}
```

#### 3.3. Update routine

##### 3.3.1. Effect of rounding errors

##### 3.3.2. Two-step method with package processing

### 4. Experimental evaluation

#### 4.1. Experiments on synthetic dataset

#### 4.2. Experiments on real-world datasets

#### 4.3. Comparison with other recent parallel k-means implementations

### 5. Conclusion

---

## Presentation plan

### ~~Subject(key points) to explain...~~ Split into 3

- Theory (Hard part)
    1. Spectral clustering ([2.1.](#21-background-for-spectral-clustering))
    2. k-Means ([2.2.](#22-k-means-algorithm))
    3. Hybrid CPU-GPU complete processing chain ([2.3.](#23-hybrid-cpu-gpu-complete-processing-chain))
- Practical (Long part)
  - Bunch of codes...
- Experiment and result, conclusion (Maybe easy, but quite important)
  - Bunch of tables and numbers...
